{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " # Perceptron Algorithm Project"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Classification of Music genre\n",
    "\n",
    "### Dataset description\n",
    "\n",
    "A music genre is a conventional category that identifies pieces of music as belonging to a shared tradition or set of conventions. It is to be distinguished from musical form and musical style. The features extracted from these songs can help the machine to assing them to the two genres. \n",
    "\n",
    "This dataset is a subset of the dataset provided [here](https://www.kaggle.com/insiyeah/musicfeatures), containing only the data regarding the classical and metal genres.\n",
    "\n",
    "### We consider 3 features for the classification\n",
    "\n",
    "1) **tempo**, the speed at which a passage of music is played, i.e., the beats per minute of the musical piece<br>\n",
    "2) **chroma_stft**, [mean chromagram activation on Short-Time Fourier Transform](https://librosa.org/doc/0.7.0/generated/librosa.feature.chroma_stft.html)<br>\n",
    "3) **spectral_centroid**, Indicates where the \"center of mass\" of the spectrum is located, i.e., it is the weighted average of the frequency transform<br>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We first import all the packages that are needed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import csv\n",
    "import numpy as np\n",
    "import scipy as sp\n",
    "import sklearn as sl\n",
    "from scipy import stats\n",
    "from sklearn import datasets\n",
    "from sklearn import linear_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Perceptron\n",
    "\n",
    "It's an **ITERATIVE, SUPERVISED LEARNING** algorithm introduced by Rosenblatt in 1958. The terget is to find the separating hyperplane (hence a linear classifier) that classifies the dataset. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![algorithm](images/algorithm.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set the random seed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "IDnumber = 22603\n",
    "np.random.seed(IDnumber)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load the dataset and use 75% of it for training the model, 25% to test it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Header: ['tempo', 'chroma_stft', 'spectral_centroid', 'label']\n",
      "\n",
      "Data shape: (200, 4)\n",
      "\n",
      "Dataset Example:\n",
      "[['92.28515625' '0.22373830597598895' '2192.798091164326' '0']\n",
      " ['161.4990234375' '0.2841730455239421' '1534.0649775815205' '0']\n",
      " ['143.5546875' '0.20811288763962318' '1396.8242648287155' '0']\n",
      " ['95.703125' '0.31289954089595506' '1680.0882644413368' '0']\n",
      " ['123.046875' '0.25857228884109024' '1173.6583080518985' '0']]\n"
     ]
    }
   ],
   "source": [
    "filename = 'data/music.csv'\n",
    "music = csv.reader(open(filename, newline='\\n'), delimiter=',')\n",
    "\n",
    "header = next(music) # skip first line\n",
    "print(f\"Header: {header}\\n\")\n",
    "\n",
    "dataset = np.array(list(music))\n",
    "print(f\"Data shape: {dataset.shape}\\n\")\n",
    "print(\"Dataset Example:\")\n",
    "print(dataset[:5,...])\n",
    "\n",
    "X = dataset[:,:-1].astype(float) #columns 0,1,2 contain the features\n",
    "Y = dataset[:,-1].astype(int)    # last column contains the labels\n",
    "\n",
    "Y = 2*Y-1                        # for the perceptron classical--> -1, metal-->1\n",
    "m = dataset.shape[0]\n",
    "#print(\"\\nNumber of samples loaded:\", m)\n",
    "permutation = np.random.permutation(m) # random permutation\n",
    "\n",
    "X = X[permutation]\n",
    "Y = Y[permutation]\n",
    "\n",
    "#print(X)\n",
    "#print(Y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Divide the data into training set and test set (75% of the data in the first set, 25% in the second one)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Number of classical instances in test: 28\n",
      "Number of metal instances in test: 22\n",
      "Shape of training set: (150, 3)\n",
      "Shape of test set: (50, 3)\n"
     ]
    }
   ],
   "source": [
    "training_portion = round(m*0.75)    \n",
    "\n",
    "# X_training = instances for training set\n",
    "X_training = X[:training_portion,:]\n",
    "#Y_training = labels for the training set\n",
    "Y_training = Y[:training_portion]\n",
    "# number of samples in the training set\n",
    "m_training = len(Y_training)\n",
    "\n",
    "# X_test = instances for test set\n",
    "X_test = X[training_portion:m, :]\n",
    "# Y_test = labels for the test set\n",
    "Y_test = Y[training_portion:m] \n",
    "# m_test needs to be the number of samples in the test set\n",
    "m_test = len(Y_test)\n",
    "\n",
    "print(\"\\nNumber of classical instances in test:\", np.sum(Y_test==-1))\n",
    "print(\"Number of metal instances in test:\", np.sum(Y_test==1))\n",
    "\n",
    "print(\"Shape of training set: \" + str(X_training.shape))\n",
    "print(\"Shape of test set: \" + str(X_test.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We add a 1 in front of each sample so that we can use a vector in homogeneous coordinates to describe all the coefficients of the model. This can be done with the function $hstack$ in $numpy$.\n",
    "Why?\n",
    "They have the advantage that the coordinates of points, including points at infinity, can be represented using finite coordinates. Formulas involving homogeneous coordinates are often simpler and more symmetric than their Cartesian counterparts. [Source: Wikipedia]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set in homogeneous coordinates:\n",
      "[[1.00000000e+00 1.84570312e+02 2.89978727e-01 1.95304853e+03]\n",
      " [1.00000000e+00 1.23046875e+02 2.58572289e-01 1.17365831e+03]\n",
      " [1.00000000e+00 1.12347147e+02 5.55233165e-01 2.69957385e+03]\n",
      " [1.00000000e+00 1.84570312e+02 5.41847119e-01 3.04839150e+03]\n",
      " [1.00000000e+00 1.35999178e+02 4.65477533e-01 2.38467428e+03]\n",
      " [1.00000000e+00 9.22851562e+01 3.80150944e-01 2.32371559e+03]\n",
      " [1.00000000e+00 1.29199219e+02 3.46797523e-01 1.87864457e+03]\n",
      " [1.00000000e+00 1.51999081e+02 4.76896884e-01 2.92283855e+03]\n",
      " [1.00000000e+00 1.35999178e+02 4.69079028e-01 2.25428142e+03]\n",
      " [1.00000000e+00 1.12347147e+02 2.15063800e-01 1.36040835e+03]]\n",
      "Training Set shape:  (150, 4)\n"
     ]
    }
   ],
   "source": [
    "# Add a 1 to each sample (homogeneous coordinates)\n",
    "\n",
    "tmp_ones = np.ones((m_training,1))              #creates a vertical 1s vector\n",
    "X_training = np.hstack((tmp_ones, X_training))  #stacks it horizontally .. \"homogeneous coordinates..\"\n",
    "\n",
    "X_test = np.hstack((np.ones((m_test,1)),X_test))\n",
    "\n",
    "print(\"Training set in homogeneous coordinates:\")\n",
    "print(X_training[:10])\n",
    "print(\"Training Set shape: \", X_training.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since the perceptron does not terminate if the data is not linearly separable, this implementation returns the desired output (see below) if it reached the termination condition (perfectly classifies all the data) or if a maximum number of iterations have already been run, where one iteration corresponds to one update of the perceptron weights. In case the termination is reached because the maximum number of iterations have been completed, the implementation should return **the best model** seen up to then.\n",
    "\n",
    "The input parameters to pass are:\n",
    "- $X$: the matrix of input features, one row for each sample\n",
    "- $Y$: the vector of labels for the input features matrix X\n",
    "- $max\\_num\\_iterations$: the maximum number of iterations for running the perceptron\n",
    "\n",
    "The output values are:\n",
    "- $best\\_w$: the vector with the coefficients of the best model\n",
    "- $best\\_error$: the *fraction* of misclassified samples for the best model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def perceptron_update(current_w, x, y):\n",
    "    \n",
    "    new_w = current_w + 0.01*x*y\n",
    "    \n",
    "    return new_w\n",
    "\n",
    "def perceptron(X, Y, max_num_iterations):\n",
    "    \n",
    "    curr_w = np.zeros(X.shape[1]) # init the w vector\n",
    "    best_w = np.zeros(X.shape[1]) # keep track of the best solution\n",
    "    num_samples = X.shape[0]\n",
    "    best_error = 2 #init\n",
    "    curr_error = 1 #init\n",
    "    \n",
    "    index_misclassified = -2  \n",
    "    num_misclassified = 0 \n",
    "    \n",
    "    #main loop continue until all samples correctly classified or max # iterations reached\n",
    "    num_iter = 1\n",
    "    \n",
    "    while ((index_misclassified != -1) and (num_iter < max_num_iterations)):\n",
    "        \n",
    "        index_misclassified = -1 # if at the end of the loop it's still -1 .. no misclass. found!\n",
    "        num_misclassified = 0    # counter for the error\n",
    "        \n",
    "        # avoid working always on the same sample, you can use a random permutation or randomize the choice of misclassified\n",
    "        permutation = np.random.permutation(num_samples) # random permutation\n",
    "        X = X[permutation]\n",
    "        Y = Y[permutation]\n",
    "        # print(\"Test permutation: \", X.shape, \" \", Y.shape)\n",
    "        \n",
    "        for i in range(num_samples):\n",
    "            check_sum = np.sum(X[i] * curr_w)      # Sum of the elements averaged by the vector W\n",
    "            if Y[i] * check_sum <= 0:                     \n",
    "                num_misclassified += 1\n",
    "                index_misclassified = i            \n",
    "        \n",
    "        if index_misclassified == -1 : \n",
    "            print(\"No errors found, the algorithm classified perfectly the training sample\")\n",
    "            break\n",
    "            \n",
    "        #update  error count, keep track of best solution\n",
    "        curr_error = num_misclassified / num_samples\n",
    "        if curr_error < best_error:\n",
    "            best_error = curr_error\n",
    "            best_w = curr_w\n",
    "            \n",
    "        num_iter += 1\n",
    "        if num_iter == max_num_iterations : print(\"Max iteration exceded\")\n",
    "        \n",
    "        #call update function using a misclassifed sample\n",
    "        if index_misclassified != -1: #a misclassification has been found.. otherwise no update\n",
    "            curr_w = perceptron_update(curr_w, X[index_misclassified], Y[index_misclassified]) \n",
    "    \n",
    "    return best_w, best_error"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we use the implementation above of the perceptron to learn a model from the training data using 100 iterations and print the error of the best model we have found."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max iteration exceded\n",
      "Training Error of perceptron (100) 0.10666666666666667\n"
     ]
    }
   ],
   "source": [
    "# We pass as max iterations 100\n",
    "iterations = 100\n",
    "w_found, error = perceptron(X_training,Y_training, iterations)\n",
    "#print(w_found)\n",
    "print(\"Training Error of perceptron (\" + str(iterations) + \") \" + str(error))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**TO DO** use the best model $w\\_found$ to predict the labels for the test dataset and print the fraction of misclassified samples in the test set (the test error that is an estimate of the true loss)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Error of perpceptron: 0.18\n"
     ]
    }
   ],
   "source": [
    "# Now we use the w_found to make predictions on test dataset\n",
    "# Recall variables: X_test ; Y_test ; m_test\n",
    "\n",
    "num_errors = 0\n",
    "for i in range(m_test):\n",
    "    check_sum = np.sum(X_test[i] * w_found)\n",
    "    if check_sum * Y_test[i] < 0 : num_errors += 1\n",
    "    \n",
    "true_loss_estimate = num_errors/m_test  # error rate on the test set\n",
    "print(\"Test Error of perpceptron: \" + str(true_loss_estimate))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[There's a tiny difference, 0.1467 for the training error vs 0.14 in the test one (which is by the way a good result), this might lead to thinking that the training set is a \"good\" one. ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max iteration exceded\n",
      "[-4.30000000e-01 -5.49484558e+01 -9.44486703e-03  3.43129637e+00]\n",
      "Training Error of perpceptron (4000 iterations): 0.1\n",
      "Test Error of perpceptron (4000 iterations): 0.14\n"
     ]
    }
   ],
   "source": [
    "# Now run the perceptron for 3000 iterations, to see if something changes\n",
    "\n",
    "w_found, error = perceptron(X_training,Y_training, 3000)\n",
    "print(w_found)\n",
    "\n",
    "print(\"Training Error of perpceptron (4000 iterations): \" + str(error))\n",
    "\n",
    "num_errors = 0\n",
    "for i in range(m_test):\n",
    "    check_sum = np.sum(X_test[i] * w_found)\n",
    "    if check_sum * Y_test[i] < 0 : num_errors += 1\n",
    "        \n",
    "true_loss_estimate = num_errors/m_test\n",
    "        \n",
    "print(\"Test Error of perpceptron (4000 iterations): \" + str(true_loss_estimate))"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
